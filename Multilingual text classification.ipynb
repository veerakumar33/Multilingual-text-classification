{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Team members:\n",
        "\n",
        "   Nithesh N    22BAD061\n",
        "\n",
        "   Siva Nirai   22BAD096\n",
        "\n",
        "   Tharanesh R  22BAD105\n",
        "   \n",
        "   Veerakumar S 22BAD109"
      ],
      "metadata": {
        "id": "NzrKxKT_sX1n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset link:\n",
        "https://www.kaggle.com/competitions/contradictory-my-dear-watson/data\n",
        "\n",
        "Dataset Description:\n",
        "The dataset contains about the hypothesis of a statement which is given in several languages\n",
        "Eg:\n",
        "He came, he opened the door and I remember looking back and seeing the expression on his face, and I could tell that he was disappointed.\n",
        "\n",
        "Hypothesis 1:\n",
        "\n",
        "Just by the look on his face when he came through the door I just knew that he was let down.\n",
        "\n",
        "We know that this is true based on the information in the premise. So, this pair is related by entailment.\n",
        "\n",
        "Hypothesis 2:\n",
        "\n",
        "He was trying not to make us feel guilty but we knew we had caused him trouble.\n",
        "\n",
        "This very well might be true, but we canâ€™t conclude this based on the information in the premise. So, this relationship is neutral.\n",
        "\n",
        "Hypothesis 3:\n",
        "\n",
        "He was so excited and bursting with joy that he practically knocked the door off it's frame."
      ],
      "metadata": {
        "id": "-8t6rJfZsypn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "xFTofUvrsvIN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing required libraries\n"
      ],
      "metadata": {
        "id": "ehz7EI44tcve"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import unicodedata\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, SpatialDropout1D"
      ],
      "metadata": {
        "id": "0C-HxqZjrYLu"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function implies,\n",
        "\n",
        "Converts the text to lowercase to ensure uniformity in text representation.\n",
        "Strips leading and trailing whitespaces to remove unnecessary spaces.\n",
        "Normalizes the text by converting accented characters to their ASCII equivalents, and removes non-alphanumeric characters except for spaces, resulting in a clean and standardized text output."
      ],
      "metadata": {
        "id": "XfTJxITstm6-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text, language='english'):\n",
        "    text = text.lower()\n",
        "    text = text.strip()\n",
        "    text = unicodedata.normalize('NFD', text).encode('ascii', 'ignore').decode(\"utf-8\")\n",
        "    if language == 'english':\n",
        "        text = re.sub(r\"[^a-z0-9 ]\", \"\", text)\n",
        "    else:\n",
        "         text = re.sub(r\"[^\\p{L}\\p{N} ]\", \"\", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "    return text"
      ],
      "metadata": {
        "id": "s-uIo4RdrkIt"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing train and test dataset\n"
      ],
      "metadata": {
        "id": "kkBlOuOAt3PW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv(\"train.csv\")\n",
        "test_df = pd.read_csv(\"test.csv\")"
      ],
      "metadata": {
        "id": "0fNlwl_2rozd"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating objects for the function"
      ],
      "metadata": {
        "id": "RS9do-3Yt9B1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df['hypothesis'] = train_df['hypothesis'].apply(preprocess_text)\n",
        "train_df['premise'] = train_df['premise'].apply(preprocess_text)\n",
        "test_df['hypothesis'] = test_df['hypothesis'].apply(preprocess_text)\n",
        "test_df['premise'] = test_df['premise'].apply(preprocess_text)"
      ],
      "metadata": {
        "id": "SeocHDfDrtZd"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Giving the train and test statements\n"
      ],
      "metadata": {
        "id": "WiTsOvFluEHw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_texts = train_df['hypothesis'] + \" \" + train_df['premise']\n",
        "test_texts = test_df['hypothesis'] + \" \" + test_df['premise']"
      ],
      "metadata": {
        "id": "iAn46GLzrwBF"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting the labels for the data"
      ],
      "metadata": {
        "id": "Ak40SuazuRmV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels = train_df['language']\n",
        "test_labels = test_df['language']"
      ],
      "metadata": {
        "id": "NAf6nMycryTt"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encoding the train and test labels"
      ],
      "metadata": {
        "id": "wYa1cFiruWat"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "label_encoder = LabelEncoder()\n",
        "train_labels_encoded = label_encoder.fit_transform(train_labels)\n",
        "test_labels_encoded = label_encoder.transform(test_labels)"
      ],
      "metadata": {
        "id": "Xy9Hbd0wr2VF"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenizing the text"
      ],
      "metadata": {
        "id": "EnzkqQxuudSd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(train_texts)"
      ],
      "metadata": {
        "id": "01DycA_Tr7SV"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Converting it to sequences"
      ],
      "metadata": {
        "id": "IwZOB5OwugGm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_sequences = tokenizer.texts_to_sequences(train_texts)\n",
        "test_sequences = tokenizer.texts_to_sequences(test_texts)"
      ],
      "metadata": {
        "id": "DSSo33ifr-ce"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adding paded sequences"
      ],
      "metadata": {
        "id": "8GcKqmfiujCm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = max(max(len(seq) for seq in train_sequences), max(len(seq) for seq in test_sequences))\n",
        "train_sequences_padded = pad_sequences(train_sequences, maxlen=max_len, padding='pre')\n",
        "test_sequences_padded = pad_sequences(test_sequences, maxlen=max_len, padding='pre')"
      ],
      "metadata": {
        "id": "Mc3PY8R4sEFu"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating sequential model with embedding,LSTM,Dense layers"
      ],
      "metadata": {
        "id": "ccIaMJVIuo3v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential([\n",
        "    Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=100, input_length=max_len),\n",
        "    SpatialDropout1D(0.2),\n",
        "    LSTM(units=200),\n",
        "    Dense(units=len(set(train_labels)), activation='softmax')\n",
        "])"
      ],
      "metadata": {
        "id": "bXs0zbZ4sG5u"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "InO0G9E8sKuW"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training the model"
      ],
      "metadata": {
        "id": "ATG-78T2uynW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(train_sequences_padded, train_labels_encoded, epochs=10, batch_size=32, validation_split=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NxScoczJsO3W",
        "outputId": "49e3cb32-c2d0-409a-88da-cb2f3ab16e8b"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "303/303 [==============================] - 219s 717ms/step - loss: 1.0616 - accuracy: 0.6345 - val_loss: 0.7801 - val_accuracy: 0.7083\n",
            "Epoch 2/10\n",
            "303/303 [==============================] - 230s 760ms/step - loss: 0.6391 - accuracy: 0.7440 - val_loss: 0.5418 - val_accuracy: 0.7908\n",
            "Epoch 3/10\n",
            "303/303 [==============================] - 232s 767ms/step - loss: 0.5409 - accuracy: 0.7867 - val_loss: 0.5348 - val_accuracy: 0.7925\n",
            "Epoch 4/10\n",
            "303/303 [==============================] - 210s 695ms/step - loss: 0.5078 - accuracy: 0.7967 - val_loss: 0.5327 - val_accuracy: 0.7921\n",
            "Epoch 5/10\n",
            "303/303 [==============================] - ETA: 0s - loss: 0.4938 - accuracy: 0.8031"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "KeyboardInterrupt\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss, accuracy = model.evaluate(test_sequences_padded, test_labels_encoded)\n",
        "print(\"Test Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u2r6VP5Nn8k1",
        "outputId": "8294205f-0737-4feb-8f34-96063a234a3e"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "163/163 [==============================] - 30s 183ms/step - loss: 0.5640 - accuracy: 0.7811\n",
            "Test Accuracy: 0.7811356782913208\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GDNpFdCCql8-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}